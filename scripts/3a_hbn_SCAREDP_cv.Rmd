---
title: "3a: Cross-validate SCARED-P models on HBN training data"
author: "Paul A. Bloom"
date: "June 21, 2019"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

This markdown cross-validates SCARED-P models on the HBN training data (linear and logistic regressions). Model metrics are saved for each fold of cv, then best models are selected and fit using Bayesian inference in rstanarm on the entire HBN training set, to be later inspected and validated.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

# Load pacakages/Set themes
```{r}
library(tidyverse)
library(ggplot2)
library(rstanarm)
library(arm)
source('helperFunctions.R')
source('cvFunction.R')

# Plot theme
mytheme = theme_bw() + theme(panel.grid.major = element_blank(), 
                             axis.text=element_text(size=14),
                             axis.title.x=element_text(size=14,face="bold"),
                             axis.title.y=element_text(size=14, face="bold"),
                             plot.title=element_text(size=20, hjust = 0.5))
theme_set(mytheme)

# Color palette
myPal = c('#2b8cbe', '#88419d', '#238b45','#cb181d')

# To get the same random number generator across R versions (https://blog.revolutionanalytics.com/2019/05/whats-new-in-r-360.html)
if (as.numeric(R.Version()$major) >= 3 & as.numeric(R.Version()$minor) >= 6.0){ 
  RNGkind(sample.kind="Rounding")
}
```


# Load in datasets
```{r}
#complete
hbnCompleteTrain = read.csv('../cleanData/hbnTrainComplete.csv', stringsAsFactors = FALSE)
hbnCompleteTrainScaredParent = filter(hbnCompleteTrain, !is.na(scaredSumParent)) %>%
  mutate(., missingData = 'complete')

# 3nn
hbn3NNTrain = read.csv('../cleanData/hbnTrain3NN.csv', stringsAsFactors = FALSE) %>%
  mutate(., missingData = '3NN')

# 9nn
hbn9NNTrain = read.csv('../cleanData/hbnTrain9NN.csv', stringsAsFactors = FALSE) %>%
  mutate(., missingData = '9NN')
```


# Linear Regression

## Define Linear Regresson Formulas

These go into list format to be read into the cvManyLinearModels() function

```{r}
sumFormulas = list(scaredSumParent ~ sex + ageCenter + cbclGISum, 
                   scaredSumParent ~ sex + ageCenter*cbclGISum,
                   scaredSumParent ~ sex*cbclGISum + ageCenter, 
                   scaredSumParent ~ cbclGISum*ageCenter*sex)

indivFormulas = list(scaredSumParent ~ sex + ageCenter + nausea + stomachache_cramps + vomiting + constipated,
                     scaredSumParent ~ sex + nausea*ageCenter + stomachache_cramps*ageCenter + vomiting*ageCenter + constipated*ageCenter,
                     scaredSumParent ~ ageCenter + nausea*sex + stomachache_cramps*sex + vomiting*sex + constipated*sex,
                     scaredSumParent ~ nausea*sex*ageCenter + stomachache_cramps*sex*ageCenter + vomiting*sex*ageCenter + constipated*sex*ageCenter)


noGiFormulas = list(scaredSumParent ~ ageCenter,
                    scaredSumParent ~ sex,
                    scaredSumParent ~ ageCenter + sex,
                    scaredSumParent ~ ageCenter*sex)

interceptFormula = list(scaredSumParent ~ 1)
```

## Define Logistic Regression Formulas
```{r}
sumFormulasScaredBinParent = list(scaredBinParent ~ sex + ageCenter + cbclGISum, 
                   scaredBinParent ~ sex + ageCenter*cbclGISum,
                   scaredBinParent ~ sex*cbclGISum + ageCenter, 
                   scaredBinParent ~ cbclGISum*ageCenter*sex)

indivFormulasScaredBinParent = list(scaredBinParent ~ sex + ageCenter + nausea + stomachache_cramps + vomiting + constipated,
                     scaredBinParent ~ sex + nausea*ageCenter + stomachache_cramps*ageCenter + vomiting*ageCenter + constipated*ageCenter,
                     scaredBinParent ~ ageCenter + nausea*sex + stomachache_cramps*sex + vomiting*sex + constipated*sex,
                     scaredBinParent ~ nausea*sex*ageCenter + stomachache_cramps*sex*ageCenter + vomiting*sex*ageCenter + constipated*sex*ageCenter)


noGiFormulasScaredBinParent = list(scaredBinParent ~ ageCenter,
                    scaredBinParent ~ sex,
                    scaredBinParent ~ ageCenter + sex,
                    scaredBinParent ~ ageCenter*sex)

interceptFormulaScaredBinParent = list(scaredBinParent ~ 1)
```

# Run cross-val on each training set -- this will take a little while! 

The cvManyLinearModels() function (sourced from cvFunction.R) is used to cross-validate all model variants for each pipeline here. This chunk may throw a parsing failures -- the parse_number() function in the cv function gets a little upset when it can't read 'ModIntScore' as a number, but it is actually fine.
```{r}
# number of rounds to run
numRounds = 100

# multiple training sets (complete cases, 3NN imputation, 9NN imputation)
trainSets = list(hbnCompleteTrainScaredParent, hbn3NNTrain, hbn9NNTrain)

# data frame to store results of cross-val with each training set
cvScaredParentOutputs = data.frame(index = 1:length(trainSets), trainSet = c('complete', '3NN', '9NN'))

# Loop through and do CV on each training set (pipeline)
for (ii in 1:length(trainSets)){
  print(ii)
  # select the training set
  trainSet = trainSets[[ii]]
  
  # Run the crossval & save to dataframe -- using the cvManyLinearModels() custom function
  # Linear regression cv with q2 as the metric
  cvLinear = cvManyLinearModels(inputData = trainSet, outcomeColumn = 'scaredSumParent', 
                                metric = q2, numFolds = 10, cvRounds = numRounds, modType = lm,
                                sumFormulas = sumFormulas, indivFormulas = indivFormulas, 
                                noGiFormulas = noGiFormulas, interceptFormula = interceptFormula)
  
  # Logistic regression cv with cross entropy as the metric
  cvLogistic = cvManyLinearModels(inputData = trainSet, outcomeColumn = 'scaredBinParent', 
                                metric = crossEntropy, numFolds = 10, cvRounds = numRounds, modType = glm,
                                sumFormulas = sumFormulasScaredBinParent, indivFormulas = indivFormulasScaredBinParent, 
                                noGiFormulas = noGiFormulasScaredBinParent, interceptFormula = interceptFormulaScaredBinParent)
  
  # Order factor levels of model forumulations for plotting
  cvLinear[[2]]$ModelSetOrdered = ordered(cvLinear[[2]]$ModelSet, c('GI Sum Score', 'GI Indiv. Items', 'No GI Term', 'Intercept Only'))
  cvLogistic[[2]]$ModelSetOrdered = ordered(cvLogistic[[2]]$ModelSet, c('GI Sum Score', 'GI Indiv. Items', 'No GI Term', 'Intercept Only'))
  
  # Order factor levels of model specification within model formulation for plotting
  cvLinear[[2]]$model = ordered(cvLinear[[2]]$model, c('No Interactions', 'Age*GI', 'Sex*GI', 'Age*Sex*GI', 'Age', 'Sex', 'Age + Sex', 'Age*Sex', 'Intercept Only'))
  cvLogistic[[2]]$model = ordered(cvLogistic[[2]]$model, c('No Interactions', 'Age*GI', 'Sex*GI', 'Age*Sex*GI', 'Age', 'Sex', 'Age + Sex', 'Age*Sex', 'Intercept Only'))
  
  # Add cv objects to output df
  cvScaredParentOutputs$cvLinear[ii] = list(cvLinear)
  cvScaredParentOutputs$cvLogistic[ii] = list(cvLogistic)
  
  # Make plots comparing cross-val distributions
  hbnLinearRegScaredParentCV = ggplot(cvLinear[[2]], aes(x = model, y = median)) +
    geom_hline(yintercept = 0, lty = 2) +
    geom_errorbar(aes(ymin = upr80, ymax = lwr80, color = ModelSet), width = 0, lwd = 1) +
    geom_errorbar(aes(ymin = upr95, ymax = lwr95, color = ModelSet), width = .1) +
    geom_point(size = 2) +
    facet_grid(~ModelSetOrdered, scales = 'free_x') +
    theme(axis.text.x = element_text(angle =45, hjust = 1),
          legend.position = 'none',
          plot.title = element_text(hjust = 0, size = 15)) +
    labs(x = '', y = bquote(~q^2), title = 'A') +
    scale_color_manual(values = myPal)
  
  hbnLogisticRegScaredParentCV = ggplot(cvLogistic[[2]], aes(x = model, y = median)) +
    geom_errorbar(aes(ymin = upr80, ymax = lwr80, color = ModelSet), width = 0, lwd = 1) +
    geom_errorbar(aes(ymin = upr95, ymax = lwr95, color = ModelSet), width = .1) +
    geom_point(size = 2) +
    facet_grid(~ModelSet, scales = 'free_x') +
    theme(axis.text.x = element_text(angle =45, hjust = 1),
          legend.position = 'none', 
          plot.title = element_text(hjust = 0, size = 15)) +
    labs(x = '', y = 'Log Loss', title = 'A') +
    scale_color_manual(values = myPal)
  
  # Store info on which model is best, as well as direct comparisons
  bestLinearModelIndivResults = dplyr::select(cvLinear[[1]], 
                    sum = contains(paste0('Sum', as.character(cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Sum Score']))),
                    indiv = contains(paste0('Indiv', as.character(cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Indiv. Items']))),
                    noGi = contains(paste0('NoGI', as.character(cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'No GI Term']))),
                    intercept = contains('Int'))
  
  bestLogisticModelIndivResults = dplyr::select(cvLogistic[[1]], 
                    sum = contains(paste0('Sum', as.character(cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Sum Score']))),
                    indiv = contains(paste0('Indiv', as.character(cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Indiv. Items']))),
                    noGi = contains(paste0('NoGI', as.character(cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'No GI Term']))),
                    intercept = contains('Int'))
  
  # Get percentages of folds for which models were better than other models usinc percentBetter() function
  sumOverNoGiLinear = 1 - percentBetter(bestLinearModelIndivResults$noGi, bestLinearModelIndivResults$sum)
  sumOverIntLinear = 1 - percentBetter(bestLinearModelIndivResults$intercept, bestLinearModelIndivResults$sum)
  indivOverIntLinear = 1 - percentBetter(bestLinearModelIndivResults$intercept, bestLinearModelIndivResults$indiv)
  indivOverNoGiLinear = 1 - percentBetter(bestLinearModelIndivResults$noGi, bestLinearModelIndivResults$indiv)
  indivOverSumLinear = 1 - percentBetter(bestLinearModelIndivResults$sum, bestLinearModelIndivResults$indiv)
  noGiOverIntLinear = 1 - percentBetter(bestLinearModelIndivResults$intercept, bestLinearModelIndivResults$noGi)
  
  # Put model comparisons in output df
  cvScaredParentOutputs$modelComparisonsLinear[ii] = list(list('sumOverNoGi' = sumOverNoGiLinear, 'sumOverInt' = sumOverIntLinear,
                                                               'indivOverInt' = indivOverIntLinear, 'indivOverNoGi' = indivOverNoGiLinear,
                                                               'indivOverSum' = indivOverSumLinear, 'noGiOverInt' = noGiOverIntLinear))
  # Equivalent model comparisons for logistic regressions
  sumOverNoGiLogistic = percentBetter(bestLogisticModelIndivResults$noGi, bestLogisticModelIndivResults$sum)
  sumOverIntLogistic = percentBetter(bestLogisticModelIndivResults$intercept, bestLogisticModelIndivResults$sum)
  indivOverIntLogistic = percentBetter(bestLogisticModelIndivResults$intercept, bestLogisticModelIndivResults$indiv)
  indivOverNoGiLogistic = percentBetter(bestLogisticModelIndivResults$noGi, bestLogisticModelIndivResults$indiv)
  indivOverSumLogistic = percentBetter(bestLogisticModelIndivResults$sum, bestLogisticModelIndivResults$indiv)
  noGiOverIntLogistic = percentBetter(bestLogisticModelIndivResults$intercept, bestLogisticModelIndivResults$noGi)
  
  # Add logistic regression comparisons to output df
  cvScaredParentOutputs$modelComparisonsLogistic[ii] = list(list('sumOverNoGi' = sumOverNoGiLogistic, 'sumOverInt' = sumOverIntLogistic,
                                                               'indivOverInt' = indivOverIntLogistic, 'indivOverNoGi' = indivOverNoGiLogistic,
                                                               'indivOverSum' = indivOverSumLogistic, 'noGiOverInt' = noGiOverIntLogistic))  
  
  
  # Make 'within-subjects' scatter plots
  
  # Linear: GI Sum vs. No-GI
  pSumNoGiLinear = ggplot(bestLinearModelIndivResults, aes(x = sum, y = noGi)) +
  geom_hline(lty = 3, yintercept = 0) + 
  geom_vline(lty = 3, xintercept = 0) + 
  geom_point(alpha = .3) +
  geom_abline(slope = 1, intercept = 0) +
  ylim(-.05, .4) + xlim(-.05, .4) +
  theme_classic() +
  labs(x= 'GI Sum', y = 'No-GI', title = 'B') +
  annotate("text", x = .3, y = .1, label = paste0('GI model better\n ', 100*sumOverNoGiLinear, '% of folds'),
           size = 2)
  # Linear: GI Indiv. Items vs. No-GI
  pIndivNoGiLinear = ggplot(bestLinearModelIndivResults, aes(x = indiv, y = noGi)) +
    geom_hline(lty = 3, yintercept = 0) + 
    geom_vline(lty = 3, xintercept = 0) + 
    geom_point(alpha = .3) +
    geom_abline(slope = 1, intercept = 0) +
    ylim(-.05, .4) + xlim(-.05, .4) +
    theme_classic() +
    labs(x= 'GI Indiv Items', y = 'No-GI', title = 'C') +
    annotate("text", x = .3, y = .1, label = paste0('GI model better\n ', 100*indivOverNoGiLinear, '% of folds'),
             size = 2)
  
  # Linear: GI Sum vs. GI Indiv. Items
  pSumIndivLinear = ggplot(bestLinearModelIndivResults, aes(y = sum, x = indiv)) +
    geom_hline(lty = 3, yintercept = 0) + 
    geom_vline(lty = 3, xintercept = 0) + 
    geom_point(alpha = .3) +
    geom_abline(slope = 1, intercept = 0) +
    ylim(-.05, .4) + xlim(-.05, .4) +
    theme_classic() +
    labs(y= 'GI Sum', x = 'GI Indiv Items', title = 'D') +
    annotate("text", x = .3, y = .1, label = paste0('Indiv. Items Better\n ', 100*indivOverSumLinear, '% of folds'),
             size = 2)
  #Logistic: GI Sum vs. No-GI
  pSumNoGiLogistic = ggplot(bestLogisticModelIndivResults, aes(x = sum, y = noGi)) +
  geom_hline(lty = 3, yintercept = 0) + 
  geom_vline(lty = 3, xintercept = 0) + 
  geom_point(alpha = .3) +
  geom_abline(slope = 1, intercept = 0) +
  ylim(.2, .8) + xlim(.2, .8) +
  theme_classic() +
  labs(x= 'GI Sum', y = 'No-GI', title = 'B') +
  annotate("text", x = .6, y = .3, label = paste0('GI model better\n ', 100*sumOverNoGiLogistic, '% of folds'),
           size = 2)
  
  # Logistic: GI Sum vs. No-GI
  pIndivNoGiLogistic = ggplot(bestLogisticModelIndivResults, aes(x = indiv, y = noGi)) +
    geom_hline(lty = 3, yintercept = 0) + 
    geom_vline(lty = 3, xintercept = 0) + 
    geom_point(alpha = .3) +
    geom_abline(slope = 1, intercept = 0) +
    ylim(.2, .8) + xlim(.2, .8) +
    theme_classic() +
    labs(x= 'GI Indiv Items', y = 'No-GI', title = 'C') +
    annotate("text", x = .6, y = .3, label = paste0('GI model better\n ', 100*indivOverNoGiLogistic, '% of folds'),
             size = 2)
  
  # Logistic: GI Sum vs. No-GI
  pSumIndivLogistic = ggplot(bestLogisticModelIndivResults, aes(y = sum, x = indiv)) +
    geom_hline(lty = 3, yintercept = 0) + 
    geom_vline(lty = 3, xintercept = 0) + 
    geom_point(alpha = .3) +
    geom_abline(slope = 1, intercept = 0) +
    ylim(.2, .8) + xlim(.2, .8) +
    theme_classic() +
    labs(y= 'GI Sum', x = 'GI Indiv Items', title = 'D') +
    annotate("text", x = .6, y = .3, label = paste0('Indiv. Items Better\n ', 100*indivOverSumLogistic, '% of folds'),
             size = 2)
  
  # Put all plots together and save pdf -- both linear and logistic
  pdf(paste0('../plots/hbnLinearRegScaredParentCV_', trainSet$missingData[1], '.pdf'), height = 6, width = 8)
  gridExtra::grid.arrange(hbnLinearRegScaredParentCV, pSumNoGiLinear, pIndivNoGiLinear, pSumIndivLinear, 
                          layout_matrix = rbind(c(1,1,1),c(2,3,4)))
  dev.off()
  
  
  pdf(paste0('../plots/hbnLogisticRegScaredParentCV_', trainSet$missingData[1], '.pdf'), height = 6, width = 8)
  gridExtra::grid.arrange(hbnLogisticRegScaredParentCV, pSumNoGiLogistic, pIndivNoGiLogistic, pSumIndivLogistic, 
                          layout_matrix = rbind(c(1,1,1),c(2,3,4)))
  dev.off()
}


# Save cross-val outputs
save(cvScaredParentOutputs, file = '../output/cvScaredParentOutputs.rda', compress = TRUE)
```

# Fit best models in each group to the entire training set

Fit full rstanarm models of the variations that performed best in cross-validation (best median model performance)

```{r}
# number of chains to use for each full bayesian model, and number of cores used to fit them
nChains = 4
nCores = 4
```
## LINEAR 

## COMPLETE
```{r, results='hide'}
# Linear models for complete-cases pipeline
cvLinear = cvScaredParentOutputs$cvLinear[[1]]

# Set up dataframe to hold model outputs
completeScaredParentLinearModFrame = data.frame(index = 1:4, modType = c('Sum', 'Indiv', 'NoGi', 'Int'))

# Pull corresponding formula for the best model from CV, fit using rstanarm::stan_glm()
completeScaredParentLinearModFrame$modObject[1] = list(stan_glm(data = trainSets[[1]],
                                         formula = sumFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Sum Score']]],
                                         chains = nChains, cores = nCores))

completeScaredParentLinearModFrame$modObject[2] = list(stan_glm(data = trainSets[[1]], 
                                         formula = indivFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Indiv. Items']]],
                                         chains = nChains, cores = nCores))

completeScaredParentLinearModFrame$modObject[3] = list(stan_glm(data = trainSets[[1]],
                                         formula = noGiFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'No GI Term']]], 
                                         chains = nChains, cores = nCores))

completeScaredParentLinearModFrame$modObject[4] =list(stan_glm(data = trainSets[[1]],
                                         formula = interceptFormula[[1]], 
                                         chains = nChains, cores = nCores))
```

## 3NN
```{r, results='hide'}
# Linear models for 3NN pipeline
cvLinear = cvScaredParentOutputs$cvLinear[[2]]

# Set up dataframe to hold model outputs
threeNNScaredParentLinearModFrame = data.frame(index = 1:4, modType = c('Sum', 'Indiv', 'NoGi', 'Int'))

threeNNScaredParentLinearModFrame$modObject[1] = list(stan_glm(data = trainSets[[2]],
                                         formula = sumFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Sum Score']]],
                                         chains = nChains, cores = nCores))

threeNNScaredParentLinearModFrame$modObject[2] = list(stan_glm(data = trainSets[[2]], 
                                         formula = indivFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Indiv. Items']]],
                                         chains = nChains, cores = nCores))

threeNNScaredParentLinearModFrame$modObject[3] = list(stan_glm(data = trainSets[[2]],
                                         formula = noGiFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'No GI Term']]], 
                                         chains = nChains, cores = nCores))

threeNNScaredParentLinearModFrame$modObject[4] =list(stan_glm(data = trainSets[[2]],
                                         formula = interceptFormula[[1]], 
                                         chains = nChains, cores = nCores))
```

## 9NN
```{r, results='hide'}
# Linear models for 3NN pipeline
cvLinear = cvScaredParentOutputs$cvLinear[[3]]

# Set up dataframe to hold model outputs
NineNNScaredParentLinearModFrame = data.frame(index = 1:4, modType = c('Sum', 'Indiv', 'NoGi', 'Int'))

NineNNScaredParentLinearModFrame$modObject[1] = list(stan_glm(data = trainSets[[3]],
                                         formula = sumFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Sum Score']]],
                                         chains = nChains, cores = nCores))

NineNNScaredParentLinearModFrame$modObject[2] = list(stan_glm(data = trainSets[[3]], 
                                         formula = indivFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'GI Indiv. Items']]],
                                         chains = nChains, cores = nCores))

NineNNScaredParentLinearModFrame$modObject[3] = list(stan_glm(data = trainSets[[3]],
                                         formula = noGiFormulas[[cvLinear[[3]]$modelNum[cvLinear[[3]]$ModelSet == 'No GI Term']]], 
                                         chains = nChains, cores = nCores))

NineNNScaredParentLinearModFrame$modObject[4] =list(stan_glm(data = trainSets[[3]],
                                         formula = interceptFormula[[1]], 
                                         chains = nChains, cores = nCores))
```

# LOGISTIC

Do the same thing fitting rstanarm models, but this time with the cross-validated logistic regressions

## COMPLETE
```{r, results='hide'}
cvLogistic = cvScaredParentOutputs$cvLogistic[[1]]
completeScaredParentLogisticModFrame = data.frame(index = 1:4, modType = c('Sum', 'Indiv', 'NoGi', 'Int'))

completeScaredParentLogisticModFrame$modObject[1] = list(stan_glm(data = trainSets[[1]],
                                         formula = sumFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Sum Score']]],
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

completeScaredParentLogisticModFrame$modObject[2] = list(stan_glm(data = trainSets[[1]], 
                                         formula = indivFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Indiv. Items']]],
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

completeScaredParentLogisticModFrame$modObject[3] = list(stan_glm(data = trainSets[[1]],
                                         formula = noGiFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'No GI Term']]], 
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

completeScaredParentLogisticModFrame$modObject[4] =list(stan_glm(data = trainSets[[1]],
                                         formula = interceptFormulaScaredBinParent[[1]], 
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))
```

## 3NN
```{r, results='hide'}
cvLogistic = cvScaredParentOutputs$cvLogistic[[2]]
threeNNScaredParentLogisticModFrame = data.frame(index = 1:4, modType = c('Sum', 'Indiv', 'NoGi', 'Int'))

threeNNScaredParentLogisticModFrame$modObject[1] = list(stan_glm(data = trainSets[[2]],
                                         formula = sumFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Sum Score']]],
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

threeNNScaredParentLogisticModFrame$modObject[2] = list(stan_glm(data = trainSets[[2]], 
                                         formula = indivFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Indiv. Items']]],
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

threeNNScaredParentLogisticModFrame$modObject[3] = list(stan_glm(data = trainSets[[2]],
                                         formula = noGiFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'No GI Term']]], 
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

threeNNScaredParentLogisticModFrame$modObject[4] =list(stan_glm(data = trainSets[[2]],
                                         formula = interceptFormulaScaredBinParent[[1]], 
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))
```

## 9NN
```{r, results='hide'}
cvLogistic = cvScaredParentOutputs$cvLogistic[[3]]
NineNNScaredParentLogisticModFrame = data.frame(index = 1:4, modType = c('Sum', 'Indiv', 'NoGi', 'Int'))

NineNNScaredParentLogisticModFrame$modObject[1] = list(stan_glm(data = trainSets[[3]],
                                         formula = sumFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Sum Score']]],
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

NineNNScaredParentLogisticModFrame$modObject[2] = list(stan_glm(data = trainSets[[3]], 
                                         formula = indivFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'GI Indiv. Items']]],
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

NineNNScaredParentLogisticModFrame$modObject[3] = list(stan_glm(data = trainSets[[3]],
                                         formula = noGiFormulasScaredBinParent[[cvLogistic[[3]]$modelNum[cvLogistic[[3]]$ModelSet == 'No GI Term']]], 
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))

NineNNScaredParentLogisticModFrame$modObject[4] =list(stan_glm(data = trainSets[[3]],
                                         formula = interceptFormulaScaredBinParent[[1]], 
                                         chains = nChains, cores = nCores, family = binomial(link = 'logit')))
```

## Get bayesian R^2 equivalent for each linear model
```{r}
completeScaredParentLinearModFrame = completeScaredParentLinearModFrame%>%
  group_by(modType) %>%
  mutate(., bayesR2 = median(bayes_R2(modObject[[1]])))


threeNNScaredParentLinearModFrame = threeNNScaredParentLinearModFrame %>%
  group_by(modType) %>%
  mutate(., bayesR2 = median(bayes_R2(modObject[[1]])))

NineNNScaredParentLinearModFrame = NineNNScaredParentLinearModFrame %>%
  group_by(modType) %>%
  mutate(., bayesR2 = median(bayes_R2(modObject[[1]])))

head(completeScaredParentLinearModFrame)
head(threeNNScaredParentLinearModFrame)
head(NineNNScaredParentLinearModFrame)
```

# Save all models out
```{r}
save(completeScaredParentLinearModFrame, threeNNScaredParentLinearModFrame, NineNNScaredParentLinearModFrame, 
     completeScaredParentLogisticModFrame, threeNNScaredParentLogisticModFrame, NineNNScaredParentLogisticModFrame,
     file = '../output/scaredParentModels.rda')
```

# Session Info
```{r}
sessionInfo()
```